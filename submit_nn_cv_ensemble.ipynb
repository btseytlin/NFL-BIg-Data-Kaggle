{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport seaborn as sns\nimport datetime\nimport tqdm\nimport re\nimport time\nfrom string import punctuation\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\n\nimport torch, torch.nn as nn\nimport torch.nn.functional as F\n\nsns.set_style('darkgrid')\nmpl.rcParams['figure.figsize'] = [15,10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from kaggle.competitions import nflrush","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Utils"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom string import punctuation\nimport datetime\nimport re\n\ndef crps(y_true, y_pred):\n    return np.mean(np.square(y_true - y_pred), axis=1)\n\ndef yard_to_cdf(yard):\n    yard = np.round(yard).astype(int)\n    indices = yard+99\n    cdfs = np.zeros((yard.shape[0], 199))\n    for i in range(len(cdfs)):\n        cdfs[i, indices[i]:] = 1\n    return cdfs\n\ndef cdf_to_yard(cdf):\n    yard_index = (cdf==1).argmax(axis=1)\n    yard = yard_index-99\n    return yard\n\n\ndef cdf_to_yard_torch(cdf):\n    yard_index = torch.sum((torch.as_tensor(cdf) <= 0),dim=1)\n    yard = yard_index-99\n    return yard\n\ndef crps_torch(y_true, y_pred):\n    y_true = torch.as_tensor(y_true)\n    y_pred = torch.as_tensor(y_pred)\n    return torch.mean((y_true - y_pred).pow(2), dim=1)\n\ndef crps_loss(y_true, y_pred_pdf):\n    y_pred_cdf = torch.cumsum(torch.as_tensor(y_pred_pdf), dim=1)\n    return crps_torch(y_true, y_pred_cdf).mean()\n\ndef crps_loss_cdf(y_true, y_pred_cdf):\n    return crps_torch(y_true, y_pred_cdf).mean()\n    \ndef clean_StadiumType(txt):\n    if pd.isna(txt):\n        return np.nan\n    txt = txt.lower()\n    txt = ''.join([c for c in txt if c not in punctuation])\n    txt = re.sub(' +', ' ', txt)\n    txt = txt.strip()\n    txt = txt.replace('outside', 'outdoor')\n    txt = txt.replace('outdor', 'outdoor')\n    txt = txt.replace('outddors', 'outdoor')\n    txt = txt.replace('outdoors', 'outdoor')\n    txt = txt.replace('oudoor', 'outdoor')\n    txt = txt.replace('indoors', 'indoor')\n    txt = txt.replace('ourdoor', 'outdoor')\n    txt = txt.replace('retractable', 'rtr.')\n    return txt\n\ndef transform_StadiumType(txt):\n    if pd.isna(txt):\n        return np.nan\n    if 'outdoor' in txt or 'open' in txt:\n        return 1\n    if 'indoor' in txt or 'closed' in txt:\n        return 0\n    return np.nan\n\ndef str_to_seconds(txt):\n    txt = txt.split(':')\n    ans = int(txt[0])*60 + int(txt[1]) + int(txt[2])/60\n    return ans\n\ndef str_to_float(txt):\n    try:\n        return float(txt)\n    except Exception as e:\n        return np.NaN\n\ndef map_weather(txt):\n    ans = 1\n    if pd.isna(txt):\n        return 0\n    if 'partly' in txt:\n        ans*=0.5\n    if 'climate controlled' in txt or 'indoor' in txt:\n        return ans*3\n    if 'sunny' in txt or 'sun' in txt:\n        return ans*2\n    if 'clear' in txt:\n        return ans\n    if 'cloudy' in txt:\n        return -ans\n    if 'rain' in txt or 'rainy' in txt:\n        return -2*ans\n    if 'snow' in txt:\n        return -3*ans\n    return 0\n\ndef new_orientation(angle, play_direction):\n    if play_direction == 0:\n        new_angle = 360.0 - angle\n        if new_angle == 360.0:\n            new_angle = 0.0\n        return new_angle\n    else:\n        return angle\n\ndef preprocess_features(df, fillna=False):\n    \"\"\"Accepts df like train data, returns X, y\"\"\"\n\n    # Feature engineering\n    df['DefendersInTheBox_vs_Distance'] = (df['DefendersInTheBox'] / df['Distance'])\n    df['StadiumType'] = df['StadiumType'].apply(clean_StadiumType)\n    df['StadiumTypeShort'] = df['StadiumType'].apply(transform_StadiumType)\n    df = df.drop(['StadiumType'], axis=1)\n\n    Turf = {'Field Turf':'Artificial', 'A-Turf Titan':'Artificial', 'Grass':'Natural', 'UBU Sports Speed S5-M':'Artificial', \n        'Artificial':'Artificial', 'DD GrassMaster':'Artificial', 'Natural Grass':'Natural', \n        'UBU Speed Series-S5-M':'Artificial', 'FieldTurf':'Artificial', 'FieldTurf 360':'Artificial', 'Natural grass':'Natural', 'grass':'Natural', \n        'Natural':'Natural', 'Artifical':'Artificial', 'FieldTurf360':'Artificial', 'Naturall Grass':'Natural', 'Field turf':'Artificial', \n        'SISGrass':'Artificial', 'Twenty-Four/Seven Turf':'Artificial', 'natural grass':'Natural'} \n\n    turf_type = df['Turf'].map(Turf)\n    df['TurfIsNatural'] = (turf_type == 'Natural')\n    df = df.drop(['Turf'], axis=1)\n\n    # CAREFUL. What if a new team appears?\n    map_abbr = {'ARI': 'ARZ', 'BAL': 'BLT', 'CLE': 'CLV', 'HOU': 'HST'}\n    for abb in df['PossessionTeam'].unique():\n        map_abbr[abb] = abb\n\n    def safe_map(val):\n        if map_abbr.get('val'):\n            return map_abbr[val]\n        else:\n            return val\n\n    df['PossessionTeam'] = df['PossessionTeam'].apply(safe_map)\n    df['HomeTeamAbbr'] = df['HomeTeamAbbr'].apply(safe_map)\n    df['VisitorTeamAbbr'] = df['VisitorTeamAbbr'].apply(safe_map)\n\n    df['HomePossesion'] = (df['PossessionTeam'] == df['HomeTeamAbbr'])\n\n    df['Field_eq_Possession'] = df['FieldPosition'] == df['PossessionTeam']\n    df['HomeField'] = df['FieldPosition'] == df['HomeTeamAbbr']\n\n    # in posession\n    df['InPosession']=(((df.Team == 'home') & (df.PossessionTeam == df.HomeTeamAbbr)) | ((df.Team == 'away') & (df.PossessionTeam == df.VisitorTeamAbbr)))\n\n    # Formation columns\n    df = pd.concat([df.drop(['OffenseFormation'], axis=1), pd.get_dummies(df['OffenseFormation'], prefix='OffenseFormation')], axis=1)\n    # Filling missing dummy columns at test stage\n    expected_columns = ['OffenseFormation_ACE',\n         'OffenseFormation_EMPTY',\n         'OffenseFormation_JUMBO',\n         'OffenseFormation_PISTOL',\n         'OffenseFormation_SHOTGUN',\n         'OffenseFormation_SINGLEBACK',\n         'OffenseFormation_WILDCAT',\n         'OffenseFormation_I_FORM']\n    for col in expected_columns:\n        if not col in df.columns:\n            df[col] = 0\n\n\n    df['GameClock'] = df['GameClock'].apply(str_to_seconds)\n\n    df['PlayerHeight'] = df['PlayerHeight'].apply(lambda x: 12*int(x.split('-')[0])+int(x.split('-')[1]))\n    df['PlayerBMI'] = 703*(df['PlayerWeight']/(df['PlayerHeight'])**2)\n\n    df['TimeHandoff'] = pd.to_datetime(df['TimeHandoff'], utc=True)\n    df['TimeSnap'] = pd.to_datetime(df['TimeSnap'], utc=True)\n    df['TimeDelta'] = (df['TimeHandoff']-df['TimeSnap']).apply(lambda x: x.total_seconds())\n    df['PlayerBirthDate'] = df['PlayerBirthDate'].apply(lambda x: datetime.datetime.strptime(x, \"%m/%d/%Y\"))\n    df['PlayerBirthDate'] = pd.to_datetime(df['PlayerBirthDate'], utc=True)\n\n    seconds_in_year = 60*60*24*365.25\n    df['PlayerAge'] = (df['TimeHandoff']-df['PlayerBirthDate']).apply(lambda x: x.total_seconds())/seconds_in_year\n    df = df.drop(['TimeHandoff', 'TimeSnap', 'PlayerBirthDate'], axis=1)\n\n    df['WindSpeed'] = df['WindSpeed'].apply(lambda x: x.lower().replace('mph', '').strip() if not pd.isna(x) else x)\n    df['WindSpeed'] = df['WindSpeed'].apply(lambda x: (int(x.split('-')[0])+int(x.split('-')[1]))/2 if not pd.isna(x) and '-' in x else x)\n    df['WindSpeed'] = df['WindSpeed'].apply(lambda x: (int(x.split()[0])+int(x.split()[-1]))/2 if not pd.isna(x) and type(x)!=float and 'gusts up to' in x else x)\n    df['WindSpeed'] = df['WindSpeed'].apply(str_to_float)\n\n    df = df.drop(['WindDirection'], axis=1)\n    df['PlayDirection'] = df['PlayDirection'].apply(lambda x: x.strip() == 'right')\n    df['IsHomeTeam'] = df['Team'].apply(lambda x: x.strip()=='home')\n\n\n\n    df['GameWeather'] = df['GameWeather'].str.lower()\n    indoor = \"indoor\"\n    df['GameWeather'] = df['GameWeather'].apply(lambda x: indoor if not pd.isna(x) and indoor in x else x)\n    df['GameWeather'] = df['GameWeather'].apply(lambda x: x.replace('coudy', 'cloudy').replace('clouidy', 'cloudy').replace('party', 'partly') if not pd.isna(x) else x)\n    df['GameWeather'] = df['GameWeather'].apply(lambda x: x.replace('clear and sunny', 'sunny and clear') if not pd.isna(x) else x)\n    df['GameWeather'] = df['GameWeather'].apply(lambda x: x.replace('skies', '').replace(\"mostly\", \"\").strip() if not pd.isna(x) else x)\n    df['GameWeather'] = df['GameWeather'].apply(map_weather)\n\n    df['IsRusher'] = df['NflId'] == df['NflIdRusher']\n    df.drop(['NflId', 'NflIdRusher'], axis=1, inplace=True)\n\n    df['X'] = df.apply(lambda row: row['X'] if row['PlayDirection'] else 120-row['X'], axis=1)\n    df['Orientation'] = df.apply(lambda row: new_orientation(row['Orientation'], row['PlayDirection']), axis=1)\n    df['Dir'] = df.apply(lambda row: new_orientation(row['Dir'], row['PlayDirection']), axis=1)\n    \n    df['YardsLeft'] = df.apply(lambda row: 100-row['YardLine'] if row['HomeField'] else row['YardLine'], axis=1)\n    df['YardsLeft'] = df.apply(lambda row: row['YardsLeft'] if row['PlayDirection'] else 100-row['YardsLeft'], axis=1)\n    \n\n    # DefensePersonnel\n    counts = []\n    for i, val in df['DefensePersonnel'].str.split(',').iteritems():\n        row = {'OL': 0, 'RB': 0, 'TE': 0, 'WR': 0, 'DL': 0, 'DB': 0, 'LB': 0, 'QB': 0}\n        if val is np.NaN:\n            counts.append({})\n            continue\n        for item in val:\n            name, number = item.strip().split(' ')[::-1]\n            row[name] = int(number)\n        counts.append(row)\n    defense_presonell_df = pd.DataFrame(counts)\n    defense_presonell_df.columns = ['defense_'+x for x in defense_presonell_df.columns]\n    defense_presonell_df = defense_presonell_df.fillna(0).astype(int)\n    defense_presonell_df.index = df.index\n    df = pd.concat([df.drop(['DefensePersonnel'], axis=1), defense_presonell_df], axis=1)\n\n\n    # OffensePersonnel\n    counts = []\n    for i, val in df['OffensePersonnel'].str.split(',').iteritems():\n        row = {'OL': 0, 'RB': 0, 'TE': 0, 'WR': 0, 'DL': 0, 'DB': 0, 'LB': 0, 'QB': 0}\n        if val is np.NaN:\n            counts.append({})\n            continue\n        for item in val:\n            name, number = item.strip().split(' ')[::-1]\n            row[name] = int(number)\n        counts.append(row)\n    offense_personnel_df = pd.DataFrame(counts)\n    offense_personnel_df.columns = ['offense_'+x for x in offense_personnel_df.columns]\n    offense_personnel_df = offense_personnel_df.fillna(0).astype(int)\n    offense_personnel_df.index = df.index\n    df = pd.concat([df.drop(['OffensePersonnel'], axis=1), offense_personnel_df], axis=1)\n    \n\n    df = sort_df(df)\n\n    if fillna:\n        df.fillna(-999, inplace=True)\n    return df\n\ndef sort_df(df):\n    df = df.sort_values(by=['PlayId', 'InPosession', 'IsRusher']).reset_index(drop=True)\n    return df\n\ndef make_x(df):\n    source_play_id = df['PlayId']\n\n    cols_delete = ['GameId', 'PlayId', 'IsRusher', 'Team']\n    df = df.drop(cols_delete, axis=1)\n\n    # Fill nan\n\n    # Text features\n    text_cols = [\n        'FieldPosition',\n         'DisplayName',\n         'PossessionTeam',\n         'PlayerCollegeName',\n         'Position',\n         'HomeTeamAbbr',\n         'VisitorTeamAbbr',\n         'Stadium',\n         'Location']\n    df = df.drop(text_cols, axis=1)\n\n    # Player features\n    cols_player = ['X',\n         'Y',\n         'S',\n         'A',\n         'Dis',\n         'Orientation',\n         'Dir',\n         'JerseyNumber',\n         'PlayerHeight',\n         'PlayerWeight',\n         'PlayerBMI',\n         'PlayerAge']\n\n    all_cols_player = np.array([[f'pl{num}_'+x for x in cols_player] for num in range(1, 23)]).flatten()\n\n    X = np.array(df[cols_player]).reshape(-1, len(cols_player)*22)\n\n    play_id_index = source_play_id[::22]\n    X_df = pd.DataFrame(X, columns=all_cols_player, index=play_id_index)\n\n    assert df[cols_player].shape[0] == X_df.shape[0] * 22\n    assert df[cols_player].shape[1] == X_df.shape[1] / 22\n\n    # Play features\n    cols_play = list(df.drop(cols_player+(['Yards'] if 'Yards' in df.columns else []), axis=1).columns)\n    X_play_col = np.zeros(shape=(X.shape[0], len(cols_play)))\n    for i, col in enumerate(cols_play):\n        X_play_col[:, i] = df[col][::22]\n\n    X_play_col_df = pd.DataFrame(X_play_col, columns=cols_play, index=play_id_index)\n    assert X_df.shape[0] == X_play_col_df.shape[0]\n    X_df = pd.concat([X_df, X_play_col_df], axis=1)\n\n    assert X_df.shape[0] == source_play_id.drop_duplicates().count()\n    return X_df\n\ndef make_y(X, df):\n    y = np.zeros(shape=(X.shape[0], 199))\n    for i, yard in enumerate(df['Yards'][::22]):\n        y[i, yard+99:] = np.ones(shape=(1, 100-yard))\n    return y\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/nfl-big-data-bowl-2020/train.csv', dtype={'WindSpeed': 'object'})\n# df_train = pd.read_csv('data/train.csv', dtype={'WindSpeed': 'object'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_preprocessed = preprocess_features(df_train, fillna=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = make_x(df_train_preprocessed)\ny_train = make_y(X_train, df_train_preprocessed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape, y_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# NN"},{"metadata":{"trusted":true},"cell_type":"code","source":"N_FEATURES = X_train.shape[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def iterate_minibatches(X, y, batchsize):\n    X = torch.as_tensor(X)\n    y = torch.as_tensor(y)\n    indices = np.random.permutation(np.arange(len(X)))\n    for start in range(0, len(indices), batchsize):\n        ix = indices[start: start + batchsize]\n        yield X[ix], y[ix]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DEVICE = 'cpu'\nBATCH_SIZE = 32\nLEARNING_RATE = 0.002\nNUM_EPOCHS = 150\nEARLY_STOP_AFTER = 10\nWEIGHT_DECAY = 0.0002\nSCHEDULER_PATIENCE = 5\nL1_LOSS_WEIGHT = 0.001","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class NeuralNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.seq = nn.Sequential(\n            nn.Linear(N_FEATURES, N_FEATURES),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.BatchNorm1d(N_FEATURES),\n            nn.Linear(N_FEATURES, N_FEATURES),\n            nn.ReLU(),\n            nn.BatchNorm1d(N_FEATURES),\n            nn.Linear(N_FEATURES, N_FEATURES),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.BatchNorm1d(N_FEATURES),\n        )\n        self.head_classifier = nn.Sequential(\n            nn.Linear(N_FEATURES, 199),\n            nn.Softmax()\n        )\n        self.head_regressor = nn.Sequential(\n            nn.Linear(N_FEATURES, 1),\n        )\n\n    def forward(self, x_batch):\n        latent_out = self.seq.forward(x_batch)\n        return self.head_classifier(latent_out), self.head_regressor(latent_out)\n\n    def loss(self, x_batch, y_batch):\n        y_batch = torch.as_tensor(y_batch).double().to(device=DEVICE)\n        y_pdf, y_yard = self(x_batch)\n        loss_crps = crps_loss(y_batch, y_pdf)\n        \n        y_yard = y_yard.flatten().double()\n        y_batch_yard = cdf_to_yard_torch(y_batch).double()\n        loss_mae = torch.mean(torch.abs(y_batch_yard-y_yard))\n        return loss_crps + loss_mae*L1_LOSS_WEIGHT\n\n    def predict_pdf(self, x_batch):\n        y_pdf, y_yard = self(x_batch)\n        return y_pdf\n    \n    def predict_cdf(self, x_batch):\n        y_pred_pdf = self.predict_pdf(x_batch)\n        y_pred_cdf = torch.cumsum(y_pred_pdf, dim=1)\n        y_pred_cdf = torch.clamp(y_pred_cdf, 0, 1)\n        return y_pred_cdf\n\nmodel = NeuralNet()\nmodel = model.double().to(device=DEVICE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train utils"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(model, opt, scheduler, X_train, y_train, X_val, y_val, early_stop_patience, batch_size, n_epochs):\n    train_loss = []\n    train_loss_stds = []\n    val_accuracy = []\n    val_accuracy_stds = []\n\n    best_val_acc = None\n    best_model = None\n    patience = 0\n\n    for epoch in range(n_epochs):\n        try:\n            opt.zero_grad()\n\n            start_time = time.time()\n            model.train(True)\n            epoch_loss = []\n\n            for X_batch, y_batch in iter(iterate_minibatches(X_train, y_train, batch_size)):\n                loss = model.loss(X_batch, y_batch)\n                epoch_loss.append(float(loss.item()))\n                loss.backward()\n                opt.step()\n                opt.zero_grad()\n\n            train_loss.append(np.mean(epoch_loss))\n            train_loss_stds.append(np.std(epoch_loss))\n            model.train(False)\n\n            epoch_val_acc = []\n            for X_batch, y_batch in iter(iterate_minibatches(X_val, y_val, batch_size)): \n                y_val_pred_cdf = model.predict_cdf(X_batch).to(device=DEVICE)\n                val_loss = crps_loss_cdf(y_batch, y_val_pred_cdf)\n                epoch_val_acc.append(val_loss.item())\n            val_acc = np.mean(epoch_val_acc)\n            val_accuracy.append(val_acc)\n            val_accuracy_stds.append(np.std(epoch_val_acc))\n\n\n            scheduler.step(val_acc)\n\n            # Then we print the results for this epoch:\n            print(\"Epoch {} of {} took {:.3f}s\".format(\n                epoch + 1, n_epochs, time.time() - start_time))\n            print(\"  training loss: \\t{:.6f}\".format(train_loss[-1]))\n            print(\"  validation score: \\t\\t\\t{:.6f}\".format(val_accuracy[-1]))\n\n            if best_val_acc is None or val_acc < best_val_acc:\n                best_val_acc = val_acc\n                patience = 0\n                torch.save(model.state_dict(), 'best_model')\n            else:\n                patience += 1\n                print(f'Validation score has not improved for {patience} epochs.')\n            if patience >= early_stop_patience:\n                print('Early stopping.')\n                break\n        except KeyboardInterrupt:\n            break\n    model.load_state_dict(torch.load('best_model'))\n    model.train(False)\n\n    history = {\n        'train_loss': train_loss,\n        'train_loss_stds': train_loss_stds,\n        'val_accuracy': val_accuracy,\n        'val_accuracy_stds': val_accuracy_stds,\n    }\n    \n    return model, history\n\ndef train_nn_pipeline(model, X, y, early_stop_after=EARLY_STOP_AFTER, batch_size=BATCH_SIZE, n_epochs=NUM_EPOCHS):\n    scaler = MinMaxScaler()\n    X = scaler.fit_transform(X)\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1)\n    \n    opt = torch.optim.Adamax(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min', patience=SCHEDULER_PATIENCE, verbose=True, threshold=1e-6, eps=1e-9)\n\n    model, train_history = train(model, opt, scheduler, X_train, y_train, X_val, y_val, early_stop_after, batch_size, n_epochs)\n    \n    return model, scaler, train_history\n\ndef nn_pipeline_predict(model, scaler, X):\n    X = torch.as_tensor(scaler.transform(X)).to(device=DEVICE)\n    y_pred = model.predict_cdf(X)\n    return y_pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CV"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RepeatedKFold\n\ncv = RepeatedKFold(n_splits=5, n_repeats=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_cv(X, y, cv, model_class, train_function):\n    models, scalers, losses = [], [], []\n    i = 0\n    X = X.values\n    for train_index, test_index in cv.split(X):\n        print(f'Training model {i}')\n        try:\n            X_train, X_test = X[train_index], X[test_index]\n            y_train, y_test = y[train_index], y[test_index]\n            model = model_class().double().to(device=DEVICE)\n            model, scaler, history = train_nn_pipeline(model, X_train, y_train)\n            y_pred_cdf_test = nn_pipeline_predict(model, scaler, X_test)\n            y_test = torch.as_tensor(y_test).to(device=DEVICE)\n            test_loss = crps_torch(y_test, y_pred_cdf_test).mean()\n            losses.append(test_loss.cpu().data.item())\n            models.append(model)\n            scalers.append(scaler)\n            i+=1\n        except KeyboardInterrupt:\n            break\n    return models, scalers, losses\n\ndef cv_predict(models, scalers, X):\n    X = X.values\n    predicted_cdfs = []\n    for model, scaler in zip(models, scalers):\n        X_prep = torch.as_tensor(scaler.transform(X)).to(device=DEVICE)\n        predicted_cdfs.append(model.predict_cdf(X_prep).cpu().data.numpy())\n    return np.mean(predicted_cdfs, axis=0) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models, scalers, losses = run_cv(X_train, y_train, cv, NeuralNet, train_nn_pipeline)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SUBMIT"},{"metadata":{"trusted":true},"cell_type":"code","source":"env = nflrush.make_env()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"def get_predictions(df_test, y_test, model):\n    X = make_x(preprocess_features(df_test, fillna=True))\n    y_pred = cv_predict(models, scalers, X)\n    return y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# preprocess_features(df_test).columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# set(make_x(df_train_preprocessed).columns).difference(set(make_x(preprocess_features(df_test, fillna=-999))))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# set(make_x(preprocess_features(df_test, fillna=True)).columns).difference(set(make_x(df_train_preprocessed).columns))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iter_test = env.iter_test()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for df_test, y_cdf_test in tqdm.tqdm(iter_test):\n    y_pred = get_predictions(df_test, y_cdf_test, model)\n    env.predict(pd.DataFrame(data=y_pred,columns=y_cdf_test.columns))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env.write_submission_file()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"}},"nbformat":4,"nbformat_minor":1}